{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vedalaxman/Audio_to_Text/blob/main/Work.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RNI13j4tXcE"
      },
      "outputs": [],
      "source": [
        "pip install google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR_YLcWCAnxE"
      },
      "outputs": [],
      "source": [
        "api_key = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uO_JwUPvh2WK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "You are an expert medical scribe. Your task is to take a raw transcription of a doctor-patient consultation and structure it according to the Indian National EHR Standard format.\n",
        "\n",
        "Analyze the following transcription and extract the information into the sections below.\n",
        "- If information for a section is not present in the transcript, write \"Not mentioned.\"\n",
        "- Do not invent any information that is not explicitly stated in the transcription.\n",
        "\n",
        "**[Patient Identification]**\n",
        "- Name:\n",
        "- Age:\n",
        "- Gender:\n",
        "\n",
        "**[Chief Complaints]**\n",
        "-\n",
        "\n",
        "**[History of Present Illness]**\n",
        "-\n",
        "\n",
        "**[Past Medical/Surgical History]**\n",
        "-\n",
        "\n",
        "**[Medication History]**\n",
        "-\n",
        "\n",
        "**[Allergies]**\n",
        "-\n",
        "\n",
        "**[Family History]**\n",
        "-\n",
        "\n",
        "**[Physical Examination Findings]**\n",
        "-\n",
        "\n",
        "**[Provisional Diagnosis / Assessment]**\n",
        "-\n",
        "\n",
        "**[Care Plan / Treatment]**\n",
        "-\n",
        "\n",
        "Here is the raw transcription to format:\n",
        "---\n",
        "{raw_transcription_text}\n",
        "---\n",
        "\"\"\"\n",
        "\n",
        "def configure_api(api_key):\n",
        "    genai.configure(api_key=api_key)\n",
        "\n",
        "def format_transcription(transcription_text: str) -> str:\n",
        "    model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "    final_prompt = PROMPT_TEMPLATE.format(raw_transcription_text=transcription_text)\n",
        "    response = model.generate_content(final_prompt)\n",
        "    return response.text\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    configure_api(api_key )\n",
        "\n",
        "    audio_file_path = \"/content/WhatsApp Audio 2025-06-10 at 18.46.02_dde6187b.mp3\"\n",
        "\n",
        "    print(f\"Uploading file: {audio_file_path}\")\n",
        "    audio_file = genai.upload_file(path=audio_file_path)\n",
        "    print(f\"Completed upload: {audio_file.name}\")\n",
        "\n",
        "    model = genai.GenerativeModel(model_name=\"gemini-2.0-flash\")\n",
        "    prompt = \"Transcribe this audio file and provide the full text.\"\n",
        "\n",
        "    print(\"Generating content...\")\n",
        "    response = model.generate_content([prompt, audio_file])\n",
        "\n",
        "    transcription_text = response.text\n",
        "\n",
        "    print(\"\\n--- Raw Transcription ---\")\n",
        "    print(transcription_text)\n",
        "\n",
        "    print(\"\\n--- Formatted EHR Output ---\")\n",
        "    formatted_output = format_transcription(transcription_text)\n",
        "    print(formatted_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "k9ZWjazUtXke"
      },
      "outputs": [],
      "source": [
        "!pip install faster-whisper\n",
        "#run this always"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1x4uTWThtXm_"
      },
      "outputs": [],
      "source": [
        "from faster_whisper import WhisperModel\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "compute_type = \"float16\" if device == \"cuda\" else \"int8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yyb3RfJ6kOdM"
      },
      "outputs": [],
      "source": [
        "model = WhisperModel(\"large\", device=device, compute_type=compute_type)\n",
        "\n",
        "print(f\"Loaded on {device} with compute_type={compute_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHHfxJ11kU7e"
      },
      "outputs": [],
      "source": [
        "segments, info = model.transcribe(\"/content/WhatsApp Audio 2025-06-10 at 18.46.02_dde6187b (1).mp3\", beam_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6or-LJScjmv"
      },
      "outputs": [],
      "source": [
        "text = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J98vk5zmkU9-"
      },
      "outputs": [],
      "source": [
        "text = []\n",
        "for segment in segments:\n",
        "    text.append(f\"{segment.text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjqX8-KAdGpU"
      },
      "outputs": [],
      "source": [
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXSd3BQmdLKw"
      },
      "outputs": [],
      "source": [
        "transcription = ' \\n'.join(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFleiaf1dQkr"
      },
      "outputs": [],
      "source": [
        "print(transcription)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sg5vWuokkVEJ"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8EIwXuvnSHn"
      },
      "outputs": [],
      "source": [
        "transcription = '''\n",
        " Please have a seat here. What's the problem?\n",
        " I have a terrible stomach ache.\n",
        " Do you have diarrhea?\n",
        " Yes, I do.\n",
        " Do you have any other symptoms?\n",
        " Yes, I feel sick.\n",
        " You mean you feel nauseous?\n",
        " That's right. I feel like vomiting.\n",
        " And right now I feel dizzy, too.\n",
        " When did the symptoms start?\n",
        " This morning. Yesterday evening I ate something raw.\n",
        " All right. Please take off your clothes to the waist and lie down there.\n",
        " Just tell me if it hurts when I do this.\n",
        " It doesn't hurt.\n",
        " Ouch! It hurts there.\n",
        " Okay. Let's hope it's just indigestion,\n",
        " but we'll need to run some diagnostic tests to be sure.\n",
        " We'll run a blood test and we'll also need a urine sample.\n",
        " Can you give me something for the time being?\n",
        " Yes, I'll give you a prescription for indigestion tablets.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itbhzrSZeq9R"
      },
      "outputs": [],
      "source": [
        "PROMPT_TEMPLATE = \"\"\"\n",
        "You are an expert medical scribe. Your task is to take a raw transcription of a doctor-patient consultation and structure it according to the Indian National EHR Standard format.\n",
        "\n",
        "Analyze the following transcription and extract the information into the sections below.\n",
        "- If information for a section is not present in the transcript, write \"Not mentioned.\"\n",
        "- Do not invent any information that is not explicitly stated in the transcription.\n",
        "\n",
        "**[Patient Identification]**\n",
        "- Name:\n",
        "- Age:\n",
        "- Gender:\n",
        "\n",
        "**[Chief Complaints]**\n",
        "-\n",
        "\n",
        "**[History of Present Illness]**\n",
        "-\n",
        "\n",
        "**[Past Medical/Surgical History]**\n",
        "-\n",
        "\n",
        "**[Medication History]**\n",
        "-\n",
        "\n",
        "**[Allergies]**\n",
        "-\n",
        "\n",
        "**[Family History]**\n",
        "-\n",
        "\n",
        "**[Physical Examination Findings]**\n",
        "-\n",
        "\n",
        "**[Provisional Diagnosis / Assessment]**\n",
        "-\n",
        "\n",
        "**[Care Plan / Treatment]**\n",
        "-\n",
        "\n",
        "Here is the raw transcription to format:\n",
        "---\n",
        "{raw_transcription_text}\n",
        "---\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36LjhwCkkVN6"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "\n",
        "model_id = \"HuggingFaceH4/zephyr-1.3b-alpha\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", trust_remote_code=True)\n",
        "\n",
        "\n",
        "prompt = PROMPT_TEMPLATE.format(raw_transcription_text=transcription)\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
        "response = generator(prompt, max_new_tokens=512, temperature=0.3)[0][\"generated_text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dva9XBrfphUw"
      },
      "outputs": [],
      "source": [
        "prompt = PROMPT_TEMPLATE.format(raw_transcription_text=transcription)\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
        "response = generator(prompt, max_new_tokens=512, temperature=0.3)[0][\"generated_text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWUIvDx8xSz9"
      },
      "outputs": [],
      "source": [
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOy5qqRKjmBl14ujTGe0AJg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}